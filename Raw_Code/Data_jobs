from bs4 import BeautifulSoup # For HTML parsing
import urllib2 # Website connections
import pandas as pd

#def text_cleaner(website):
    '''
    This function just cleans up the raw html so that I can look at it.
    Inputs: a URL to investigate
    Outputs: Cleaned text only
    '''
    
#try:
website = "https://www.yelp.com/careers/job-openings/e733f9c7-a970-4075-a403-d99fdc55675b?description=Data-Scientist-New-Grad_College-Engineering-Product_San-Francisco-CA&lever-source=indeed"
site = urllib2.urlopen(website).read() # Connect to the job posting
    #except: 
    #    return   # Need this in case the website isn't there anymore or some other weird connection problem 

soup_obj = BeautifulSoup(site) # Get the html from the site

for script in soup_obj(["script", "style"]):
    script.extract() # Remove these two elements from the BS4 object



text = soup_obj.get_text() # Get the text from this



lines = (line.strip() for line in text.splitlines()) # break into lines



chunks = (phrase.strip() for line in lines for phrase in line.split("  ")) # break multi-headlines into a line each

def chunk_space(chunk):
    chunk_out = chunk + ' ' # Need to fix spacing issue
    return chunk_out  


    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line


    # Now clean out all of the unicode junk (this line works great!!!)

    try:
        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted
    except:                                                            # in a way that this works, can occasionally throw
        return                                                         # an exception


    text = re.sub("[^a-zA-Z.+3]"," ", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)
                                                # Also include + for C++


    text = text.lower().split()  # Go to lower case and split them apart


    stop_words = set(stopwords.words("english")) # Filter out any stop words
    text = [w for w in text if not w in stop_words]



    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed
                            # or not on the website)

    return text